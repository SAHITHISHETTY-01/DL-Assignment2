# -*- coding: utf-8 -*-
"""DL Assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/114BNd-rYgtUs5D7-4p6ieaTJ67wwY-uW

Question 1
Build an RNN based seq2seq model, which contains the following layers: (i) input layer for
character embeddings (ii) one encoder RNN, which sequentially encodes the input character,
sequence (Latin) (iii) one decoder RNN which takes the last state of the encoder as input and
produces one output character at a time (Devanagari).

The code should be flexible such that the dimension of the input character embeddings, the
hidden states of the encoders and decoders, the cell (RNN, LSTM, GRU) and the number of
layers in the encoder and decoder can be changed.
(a) What is the total number of computations done by your network? (assume that the input
embedding size is mmm, encoder and decoder have 1 layer each, the hidden cell state is k for
both the encoder and decoder, the length of the input and output sequence is the same, i.e.,
T, the size of the vocabulary is the same for the source and target language, i.e., V)
(b) What is the total number of parameters in your network? (assume that the input
embedding size is mmm, encoder and decoder have 1 layer each, the hidden cell state is k for
both the encoder and decoder and the length of the input and output sequence is the same,
i.e., T, the size of the vocabulary is the same for the source and target language, i.e., V)
c) Use the best model from your sweep and report the accuracy on the test set and Provide
sample inputs from the test data and predictions made by your best model.
"""

import numpy as np
import pandas as pd
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.utils import to_categorical

# --------------------------------------
# ðŸ”¹ Load and Preprocess Data
# --------------------------------------
def load_dataset(path):
    df = pd.read_csv(path, sep='\t', header=None)
    df = df.dropna()
    input_texts = df[1].astype(str).tolist()
    target_texts = ['\t' + text + '\n' for text in df[0].astype(str)]
    return input_texts, target_texts

input_texts, target_texts = load_dataset('/content/hi.translit.sampled.train.tsv')

# Build vocabulary
input_chars = sorted(set(''.join(input_texts)))
target_chars = sorted(set(''.join(target_texts)))
input_char2idx = {ch: i for i, ch in enumerate(input_chars)}
target_char2idx = {ch: i for i, ch in enumerate(target_chars)}
input_idx2char = {i: ch for ch, i in input_char2idx.items()}
target_idx2char = {i: ch for ch, i in target_char2idx.items()}

# Sequence lengths
max_encoder_len = max(len(txt) for txt in input_texts)
max_decoder_len = max(len(txt) for txt in target_texts)

# One-hot encoding
encoder_input = np.zeros((len(input_texts), max_encoder_len, len(input_chars)), dtype='float32')
decoder_input = np.zeros((len(target_texts), max_decoder_len, len(target_chars)), dtype='float32')
decoder_output = np.zeros((len(target_texts), max_decoder_len, len(target_chars)), dtype='float32')

for i, (input_seq, target_seq) in enumerate(zip(input_texts, target_texts)):
    for t, ch in enumerate(input_seq):
        encoder_input[i, t, input_char2idx[ch]] = 1.0
    for t, ch in enumerate(target_seq):
        decoder_input[i, t, target_char2idx[ch]] = 1.0
        if t > 0:
            decoder_output[i, t - 1, target_char2idx[ch]] = 1.0

# --------------------------------------
# ðŸ”¹ Build Seq2Seq Model
# --------------------------------------
hidden_size = 256

# Encoder
enc_inputs = Input(shape=(None, len(input_chars)))
enc_lstm = LSTM(hidden_size, return_state=True)
_, state_h, state_c = enc_lstm(enc_inputs)
enc_states = [state_h, state_c]

# Decoder
dec_inputs = Input(shape=(None, len(target_chars)))
dec_lstm = LSTM(hidden_size, return_sequences=True, return_state=True)
dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=enc_states)
dec_dense = Dense(len(target_chars), activation='softmax')
dec_outputs = dec_dense(dec_outputs)

model = Model([enc_inputs, dec_inputs], dec_outputs)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

# Train
model.fit([encoder_input, decoder_input], decoder_output, batch_size=128, epochs=20, validation_split=0.2)

# --------------------------------------
# ðŸ”¹ Inference Models
# --------------------------------------
# Encoder inference
encoder_model = Model(enc_inputs, enc_states)

# Decoder inference
decoder_state_input_h = Input(shape=(hidden_size,))
decoder_state_input_c = Input(shape=(hidden_size,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

decoder_outputs, state_h_dec, state_c_dec = dec_lstm(
    dec_inputs, initial_state=decoder_states_inputs
)
decoder_states = [state_h_dec, state_c_dec]
decoder_outputs = dec_dense(decoder_outputs)

decoder_model = Model([dec_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)

# --------------------------------------
# ðŸ”¹ Transliteration Function
# --------------------------------------
def transliterate_word(word):
    input_seq = np.zeros((1, max_encoder_len, len(input_chars)))
    for t, ch in enumerate(word):
        if ch in input_char2idx:
            input_seq[0, t, input_char2idx[ch]] = 1.0

    states = encoder_model.predict(input_seq)

    target_seq = np.zeros((1, 1, len(target_chars)))
    target_seq[0, 0, target_char2idx['\t']] = 1.0

    output = ''
    for _ in range(max_decoder_len):
        predictions, h, c = decoder_model.predict([target_seq] + states)
        idx = np.argmax(predictions[0, -1, :])
        char = target_idx2char[idx]
        if char == '\n':
            break
        output += char

        target_seq = np.zeros((1, 1, len(target_chars)))
        target_seq[0, 0, idx] = 1.0
        states = [h, c]

    return output

# --------------------------------------
# ðŸ”¹ Interactive Loop
# --------------------------------------
while True:
    test_input = input("Enter a Latin word (or type 'exit'): ")
    if test_input.lower() == 'exit':
        break
    print("Predicted Devanagari:", transliterate_word(test_input))

"""Question 2:
Your task is to finetune the GPT2 model to generate lyrics for English songs. You can refer

to (https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-
huggingface-f3acb35bc86a ) and follow the steps there. This blog shows how to fine-tune

the GPT2 model to generate headlines for financial articles. Instead of headlines, you will
use lyrics so you may find the following datasets useful for training:
https://data.world/datasets/lyrics
https://www.kaggle.com/paultimothymooney/poetry


"""

!pip install datasets

import pandas as pd

file_path = '/content/Billie_Eilish_Lyrics.xlsx'
df = pd.read_excel(file_path, sheet_name='Lyrics')

import pandas as pd
df = pd.read_excel('Billie_Eilish_Lyrics.xlsx', sheet_name='Lyrics')
df_clean = df[['Lyrics']].dropna().drop_duplicates()
text_data = "\n\n".join(df_clean['Lyrics'].tolist())
with open("lyrics_data.txt", "w", encoding="utf-8") as f:
    f.write(text_data)

from datasets import Dataset
from transformers import GPT2Tokenizer
dataset = Dataset.from_dict({"text": [text_data]})
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
def tokenize_function(examples):
    return tokenizer(examples["text"], return_special_tokens_mask=True)
tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

block_size = 128

def group_texts(examples):
    concatenated = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated["input_ids"])
    total_length = (total_length // block_size) * block_size
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated.items()
    }
    return result

lm_dataset = tokenized_dataset.map(group_texts, batched=True)

import os
os.environ["WANDB_DISABLED"] = "true"

import torch
from transformers import GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments
model = GPT2LMHeadModel.from_pretrained("gpt2")
model.resize_token_embeddings(len(tokenizer))
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
training_args = TrainingArguments(
    output_dir="./gpt2-lyrics",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=500,
    save_total_limit=2,
    logging_steps=100,
    prediction_loss_only=True,
    fp16=torch.cuda.is_available(),
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)
trainer.train()

trainer.save_model("./gpt2-lyrics")
tokenizer.save_pretrained("./gpt2-lyrics")
print("Model saved to ./gpt2-lyrics")

from transformers import pipeline

generator = pipeline("text-generation", model="./gpt2-lyrics", tokenizer=tokenizer)
prompt = "I want it that way"
output = generator(prompt, max_length=100, num_return_sequences=1)[0]['generated_text']
print("Generated Lyrics:\n", output)